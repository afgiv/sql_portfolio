ğŸ“˜ Makefile Usage Guide

ğŸ“Œ Overview

This Makefile automates the setup and refresh of the Sales Data Sample ETL pipeline.

It covers:

Infrastructure (database, dimensions, fact, staging)

Automated ETL (load staging â†’ cleaning â†’ dims â†’ fact â†’ views â†’ quality checks)

Designed for repeatable and idempotent execution â€” so you can easily reset or refresh the pipeline.

âš™ï¸ Prerequisites

PostgreSQL installed and psql available in PATH

Python 3 installed (for loading staging data)

.env file configured with database details

ğŸ“ Environment Variables

An .env.example file is provided. Run this command on Git Bash:

cp .env.example .env

Open the .env file with text editor and update with your values.

ğŸ”’ Note: The database password is not stored in .env. You will be prompted when running commands.

ğŸš€ Commands (Git Bash)
1. Infrastructure Setup (run once)

Creates database and schema objects.

make infra

Equivalent to running:

make create_database â†’ Drops & recreates the database

make create_dims â†’ Creates dimension tables

make create_fact â†’ Creates fact table

make create_staging â†’ Creates staging table

2. Automated ETL Pipeline (run repeatedly)

Runs the full transformation pipeline:

make automated

Steps executed:

make load_staging â†’ Load raw data into the staging table

make cleaning â†’ Cleans staging data

make dims â†’ Loads dimension tables

make fact â†’ Loads fact table

make qc_fact â†’ Runs fact-level data quality checks

make views â†’ Creates semantic/reporting views

make qc_views â†’ Runs view-level data quality checks

3. Run Individual Targets

You can also run steps individually:

make cleaning
make dims
make fact
make qc_views

ğŸ§¹ Resetting the Database

To rebuild from scratch:

make create_database
make infra


âœ… With this setup, contributors can reproduce your pipeline simply by configuring their .env and running:

make infra
make automated